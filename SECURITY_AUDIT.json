[
    {
        "id": "SEC-001",
        "category": "Security",
        "severity": "Critical",
        "title": "No Authentication/Authorization - Multi-User Data Leakage",
        "file": "app.py",
        "lines": "35-36, 59, 344, 429",
        "description": "Global variables (CURRENT_CSV_FILE, CURRENT_MAPPING, data_cache) are shared across ALL users. Any user can access/modify another user's data.",
        "threat": "Data breach, unauthorized access, GDPR/compliance violations",
        "reproducible_test": "1. User A uploads sensitive_data.csv\n2. User B (different browser/IP) calls /api/dashboard\n3. User B receives User A's data\n4. User B can overwrite User A's mapping via /api/save-mapping",
        "impact": "Complete data isolation failure. Production deployment would expose all user data to all users.",
        "suggested_fix": "IMMEDIATE:\n1. Implement session-based or JWT authentication\n2. Add user_id to all global state: {user_id: {csv_file, mapping, cache}}\n3. Filter all API calls by authenticated user_id\n4. Use Redis with user-scoped keys for production\n\nCode example:\n```python\nfrom flask import session\nfrom functools import wraps\n\ndef require_auth(f):\n    @wraps(f)\n    def decorated(*args, **kwargs):\n        if 'user_id' not in session:\n            return jsonify({'error': 'Unauthorized'}), 401\n        return f(*args, **kwargs)\n    return decorated\n\nUSER_STATE = {}  # {user_id: {csv_file, mapping, cache}}\n\n@app.route('/api/dashboard')\n@require_auth\ndef get_dashboard():\n    user_id = session['user_id']\n    user_data = USER_STATE.get(user_id, {})\n    # ...\n```",
        "mitigation": "DO NOT deploy to production without fixing. For hackathon demo: add disclaimer that this is single-user only."
    },
    {
        "id": "SEC-002",
        "category": "Security",
        "severity": "Critical",
        "title": "Unrestricted File Upload - Remote Code Execution Risk",
        "file": "app.py",
        "lines": "342-356",
        "description": "File upload only checks .csv extension (client-controlled). Attacker can upload malicious files (e.g., .csv.php, .csv with embedded macros).",
        "threat": "Remote code execution, server compromise, malware distribution",
        "reproducible_test": "1. Create malicious.csv with embedded Python code in cell values\n2. Upload via /api/upload-csv\n3. If pandas executes code during read_csv (rare but possible with certain configs), RCE achieved\n4. Alternatively: upload huge.csv (10GB) to exhaust disk/memory",
        "impact": "Server compromise, DoS, data exfiltration",
        "suggested_fix": "```python\nimport magic  # python-magic library\nfrom werkzeug.utils import secure_filename\n\nMAX_FILE_SIZE = 50 * 1024 * 1024  # 50MB\nALLOWED_MIME_TYPES = ['text/csv', 'text/plain']\n\n@app.route('/api/upload-csv', methods=['POST'])\ndef upload_csv():\n    file = request.files['file']\n    \n    # 1. Validate file size\n    file.seek(0, os.SEEK_END)\n    size = file.tell()\n    file.seek(0)\n    if size > MAX_FILE_SIZE:\n        return jsonify({'error': 'File too large'}), 413\n    \n    # 2. Validate MIME type (not just extension)\n    mime = magic.from_buffer(file.read(1024), mime=True)\n    file.seek(0)\n    if mime not in ALLOWED_MIME_TYPES:\n        return jsonify({'error': 'Invalid file type'}), 400\n    \n    # 3. Sanitize filename\n    filename = secure_filename(file.filename)\n    \n    # 4. Scan for malicious content (basic)\n    content = file.read().decode('utf-8', errors='ignore')\n    if any(bad in content.lower() for bad in ['<script', '<?php', 'eval(']):\n        return jsonify({'error': 'Malicious content detected'}), 400\n    file.seek(0)\n```",
        "mitigation": "Add file size limit (50MB), MIME type validation, filename sanitization, content scanning."
    },
    {
        "id": "SEC-003",
        "category": "Security",
        "severity": "High",
        "title": "Path Traversal Vulnerability in File Loading",
        "file": "app.py",
        "lines": "81-87",
        "description": "load_data() constructs file paths without sanitization. Attacker can use '../' to read arbitrary files.",
        "threat": "Arbitrary file read, source code disclosure, credential theft",
        "reproducible_test": "1. Set CURRENT_CSV_FILE = '../../etc/passwd' (Linux) or '..\\\\..\\\\Windows\\\\System32\\\\config\\\\SAM' (Windows)\n2. Call /api/dashboard\n3. Server attempts to read sensitive system files",
        "impact": "Full filesystem read access, exposure of secrets, database credentials, API keys",
        "suggested_fix": "```python\nimport os\nfrom werkzeug.utils import secure_filename\n\ndef load_data(csv_filename=None):\n    if csv_filename is None:\n        csv_filename = CURRENT_CSV_FILE\n    \n    if csv_filename is None:\n        return None\n    \n    # CRITICAL: Sanitize filename\n    safe_filename = secure_filename(csv_filename)\n    if safe_filename != csv_filename:\n        print(f\"❌ Rejected unsafe filename: {csv_filename}\")\n        return None\n    \n    # Only allow files in UPLOAD_FOLDER\n    csv_path = os.path.join(UPLOAD_FOLDER, safe_filename)\n    csv_path = os.path.abspath(csv_path)  # Resolve to absolute path\n    \n    # Verify path is still within UPLOAD_FOLDER\n    if not csv_path.startswith(os.path.abspath(UPLOAD_FOLDER)):\n        print(f\"❌ Path traversal attempt: {csv_filename}\")\n        return None\n    \n    if not os.path.exists(csv_path):\n        return None\n```",
        "mitigation": "Use secure_filename(), validate paths stay within UPLOAD_FOLDER, use absolute path resolution."
    },
    {
        "id": "SEC-004",
        "category": "Security",
        "severity": "High",
        "title": "CORS Wildcard - Cross-Origin Data Theft",
        "file": "app.py",
        "lines": "25",
        "description": "CORS(app) with no restrictions allows ANY website to make requests and steal user data.",
        "threat": "CSRF attacks, data exfiltration from malicious websites",
        "reproducible_test": "1. Attacker creates evil.com with JavaScript: fetch('http://your-app.com/api/dashboard')\n2. Victim visits evil.com while logged into your app\n3. evil.com receives victim's dashboard data via CORS",
        "impact": "Data theft, CSRF attacks, session hijacking",
        "suggested_fix": "```python\nfrom flask_cors import CORS\n\n# Production:\nCORS(app, resources={\n    r\"/api/*\": {\n        \"origins\": [\"https://yourdomain.com\"],  # Whitelist only your frontend\n        \"methods\": [\"GET\", \"POST\"],\n        \"allow_headers\": [\"Content-Type\", \"Authorization\"],\n        \"supports_credentials\": True\n    }\n})\n\n# Development:\nif os.getenv('FLASK_ENV') == 'development':\n    CORS(app, origins=[\"http://localhost:3000\", \"http://localhost:5173\"])\n```",
        "mitigation": "Restrict CORS to specific origins, enable credentials, limit methods."
    },
    {
        "id": "REL-001",
        "category": "Reliability",
        "severity": "Critical",
        "title": "Race Condition in Global State Updates",
        "file": "app.py",
        "lines": "344, 376-377, 429",
        "description": "Multiple concurrent requests can corrupt global state (CURRENT_CSV_FILE, CURRENT_MAPPING, data_cache).",
        "threat": "Data corruption, inconsistent state, wrong forecasts served to users",
        "reproducible_test": "1. User A uploads file1.csv (sets CURRENT_CSV_FILE='file1.csv')\n2. Simultaneously, User B uploads file2.csv (sets CURRENT_CSV_FILE='file2.csv')\n3. User A calls /api/dashboard expecting file1 data\n4. User A receives file2 data (race condition)",
        "impact": "Wrong data served, forecast errors, business decisions based on incorrect data",
        "suggested_fix": "```python\nimport threading\n\n# Use thread-local storage\nfrom flask import g\n\n@app.before_request\ndef before_request():\n    g.user_state = USER_STATE.get(session.get('user_id'), {})\n\n# OR use locks (not recommended for production)\nstate_lock = threading.Lock()\n\n@app.route('/api/upload-csv', methods=['POST'])\ndef upload_csv():\n    with state_lock:\n        global CURRENT_CSV_FILE\n        CURRENT_CSV_FILE = filename\n```\nBetter: Move to user-scoped state (see SEC-001).",
        "mitigation": "Add threading locks OR move to user-scoped state immediately."
    },
    {
        "id": "PERF-001",
        "category": "Performance",
        "severity": "High",
        "title": "Unbounded Memory Usage - In-Memory Data Cache",
        "file": "app.py",
        "lines": "44, 74-76",
        "description": "data_cache stores full DataFrames in memory with no size limit or eviction policy.",
        "threat": "Memory exhaustion, server crash, DoS",
        "reproducible_test": "1. Upload 10 different 100MB CSV files\n2. Each creates a cache entry\n3. Server memory grows to 1GB+\n4. Server crashes with OOM error",
        "impact": "Service outage, poor performance, inability to handle multiple users",
        "suggested_fix": "```python\nfrom cachetools import LRUCache\nimport sys\n\n# Limit cache size\nMAX_CACHE_SIZE = 100  # Max 100 entries\nMAX_CACHE_MEMORY_MB = 500  # Max 500MB total\n\ndata_cache = LRUCache(maxsize=MAX_CACHE_SIZE)\n\ndef load_data(csv_filename=None):\n    # ... existing code ...\n    \n    # Check memory before caching\n    df_size_mb = df.memory_usage(deep=True).sum() / (1024 * 1024)\n    if df_size_mb > 100:  # Single DataFrame > 100MB\n        print(f\"⚠️ Large DataFrame ({df_size_mb:.1f}MB), not caching\")\n        return df  # Don't cache huge DataFrames\n    \n    data_cache[cache_key] = df\n    return df\n```\nBetter: Use Redis with TTL for production.",
        "mitigation": "Implement LRU cache with size limits, use Redis for production, add memory monitoring."
    },
    {
        "id": "PERF-002",
        "category": "Performance",
        "severity": "High",
        "title": "Synchronous ML Model Fitting Blocks Server",
        "file": "ml/forecast.py",
        "lines": "496-514",
        "description": "Prophet/ARIMA model fitting (can take 30s+) runs synchronously, blocking all other requests.",
        "threat": "Server unresponsive, request timeouts, poor user experience",
        "reproducible_test": "1. Upload large dataset (1M rows)\n2. Call /api/dashboard with horizon=12\n3. Prophet model fitting takes 60+ seconds\n4. All other API requests (from any user) timeout",
        "impact": "Single slow request blocks entire server, multi-user system unusable",
        "suggested_fix": "```python\nfrom celery import Celery\n\ncelery = Celery('tasks', broker='redis://localhost:6379/0')\n\n@celery.task\ndef generate_forecast_async(df_json, horizon):\n    df = pd.read_json(df_json)\n    return generate_ml_forecast(df, horizon)\n\n@app.route('/api/dashboard')\ndef get_dashboard():\n    # Start async task\n    task = generate_forecast_async.delay(df.to_json(), horizon)\n    \n    # Return task ID immediately\n    return jsonify({\n        'task_id': task.id,\n        'status': 'processing'\n    })\n\n@app.route('/api/forecast-status/<task_id>')\ndef forecast_status(task_id):\n    task = generate_forecast_async.AsyncResult(task_id)\n    if task.ready():\n        return jsonify({'status': 'complete', 'result': task.result})\n    return jsonify({'status': 'processing'})\n```",
        "mitigation": "Use Celery + Redis for async tasks, implement task queue, add progress indicators."
    },
    {
        "id": "SEC-005",
        "category": "Security",
        "severity": "Medium",
        "title": "Hardcoded Blockchain URL - Credential Exposure Risk",
        "file": "app.py",
        "lines": "30",
        "description": "GANACHE_URL hardcoded. If changed to mainnet, private keys could be committed to git.",
        "threat": "Credential leakage, unauthorized blockchain transactions",
        "reproducible_test": "1. Developer changes GANACHE_URL to real Ethereum node with API key\n2. Commits code to GitHub\n3. API key exposed in git history forever",
        "impact": "Financial loss, unauthorized transactions, API key theft",
        "suggested_fix": "```python\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\nGANACHE_URL = os.getenv('BLOCKCHAIN_URL', 'http://127.0.0.1:8545')\nCONTRACT_ADDRESS = os.getenv('CONTRACT_ADDRESS', '')\n\n# .env file (NOT committed to git):\n# BLOCKCHAIN_URL=https://mainnet.infura.io/v3/YOUR_API_KEY\n# CONTRACT_ADDRESS=0x...\n\n# .gitignore:\n# .env\n# *.key\n# *.pem\n```",
        "mitigation": "Use environment variables, add .env to .gitignore, use secrets management (AWS Secrets Manager, HashiCorp Vault)."
    },
    {
        "id": "REL-002",
        "category": "Reliability",
        "severity": "Medium",
        "title": "No Input Validation on Forecast Horizon",
        "file": "app.py",
        "lines": "470-475",
        "description": "forecast_horizon parameter not validated. User can send negative numbers or huge values.",
        "threat": "Server crash, infinite loops, memory exhaustion",
        "reproducible_test": "1. Call /api/dashboard?horizon=999999\n2. ML model tries to forecast 999,999 weeks into future\n3. Server runs out of memory/crashes",
        "impact": "DoS, poor UX, wasted compute resources",
        "suggested_fix": "```python\n@app.route('/api/dashboard', methods=['GET'])\ndef get_dashboard():\n    try:\n        horizon = int(request.args.get('horizon', 4))\n        \n        # Validate range\n        if horizon < 1:\n            return jsonify({'error': 'Horizon must be >= 1'}), 400\n        if horizon > 52:  # Max 1 year\n            return jsonify({'error': 'Horizon must be <= 52 weeks'}), 400\n        \n    except ValueError:\n        return jsonify({'error': 'Invalid horizon value'}), 400\n```",
        "mitigation": "Add input validation: 1 <= horizon <= 52, validate all user inputs."
    },
    {
        "id": "PERF-003",
        "category": "Performance",
        "severity": "Medium",
        "title": "Inefficient Date Filtering - Full Table Scan",
        "file": "app.py",
        "lines": "466-471",
        "description": "Date filtering uses pandas boolean indexing on full DataFrame (O(n) scan) instead of indexed lookup.",
        "threat": "Slow queries on large datasets, poor scalability",
        "reproducible_test": "1. Upload 1M row CSV\n2. Filter to 1-week date range\n3. Query takes 5+ seconds (should be <100ms)",
        "impact": "Poor performance, timeouts, bad UX",
        "suggested_fix": "```python\n# Option 1: Set date as index\ndf = df.set_index('InvoiceDate').sort_index()\ndf_filtered = df.loc[from_dt:to_dt]\n\n# Option 2: Use query() for better performance\ndf_filtered = df.query('InvoiceDate >= @from_dt and InvoiceDate <= @to_dt')\n\n# Option 3: Move to SQL database\n# SELECT * FROM transactions WHERE date BETWEEN ? AND ?\n# (Uses index, 1000x faster)\n```",
        "mitigation": "Set date column as index, use df.query(), or migrate to SQL database for large datasets."
    },
    {
        "id": "MAINT-001",
        "category": "Maintainability",
        "severity": "Medium",
        "title": "Timeout Decorator Uses Unix Signals - Windows Incompatible",
        "file": "ml/forecast.py",
        "lines": "39-66",
        "description": "signal.SIGALRM not available on Windows. Timeout decorator will crash on Windows servers.",
        "threat": "Production deployment failure on Windows, no timeout protection",
        "reproducible_test": "1. Run app on Windows Server\n2. Upload dataset\n3. Server crashes with: AttributeError: module 'signal' has no attribute 'SIGALRM'",
        "impact": "Windows deployment broken, no cross-platform support",
        "suggested_fix": "```python\nimport platform\nfrom threading import Thread, Event\n\ndef timeout(seconds=20):\n    def decorator(func):\n        @wraps(func)\n        def wrapper(*args, **kwargs):\n            result = [None]\n            exception = [None]\n            \n            def target():\n                try:\n                    result[0] = func(*args, **kwargs)\n                except Exception as e:\n                    exception[0] = e\n            \n            thread = Thread(target=target)\n            thread.daemon = True\n            thread.start()\n            thread.join(timeout=seconds)\n            \n            if thread.is_alive():\n                # Thread still running = timeout\n                logger.warning(f\"{func.__name__} timed out after {seconds}s\")\n                raise TimeoutError(f\"Function timed out after {seconds}s\")\n            \n            if exception[0]:\n                raise exception[0]\n            \n            return result[0]\n        return wrapper\n    return decorator\n```",
        "mitigation": "Use threading-based timeout (cross-platform) or multiprocessing with timeout."
    },
    {
        "id": "SEC-006",
        "category": "Security",
        "severity": "Low",
        "title": "Debug Mode Enabled - Information Disclosure",
        "file": "app.py",
        "lines": "814 (if app.run(debug=True) exists)",
        "description": "Flask debug mode exposes stack traces, source code, and interactive debugger to attackers.",
        "threat": "Source code disclosure, remote code execution via debugger PIN",
        "reproducible_test": "1. Trigger any error in production\n2. Attacker sees full stack trace with file paths, variable values\n3. Attacker can access interactive debugger if they guess/brute-force PIN",
        "impact": "Information leakage, potential RCE",
        "suggested_fix": "```python\nimport os\n\nif __name__ == '__main__':\n    debug_mode = os.getenv('FLASK_ENV') == 'development'\n    app.run(\n        host='0.0.0.0',\n        port=5000,\n        debug=debug_mode  # Only True in development\n    )\n```",
        "mitigation": "Never use debug=True in production, use environment variables, implement proper error handling."
    },
    {
        "id": "SCALE-001",
        "category": "Scalability",
        "severity": "High",
        "title": "Single-Process Flask Server - No Horizontal Scaling",
        "file": "app.py",
        "lines": "All",
        "description": "Flask development server is single-threaded. Cannot handle concurrent requests or scale horizontally.",
        "threat": "Poor performance under load, inability to scale for production",
        "reproducible_test": "1. Send 10 concurrent requests to /api/dashboard\n2. Requests are processed sequentially (10-60s total)\n3. Should be parallel (<10s total)",
        "impact": "Cannot handle production traffic, poor performance, no fault tolerance",
        "suggested_fix": "```bash\n# Production deployment:\n# 1. Use Gunicorn (WSGI server)\npip install gunicorn\ngunicorn -w 4 -b 0.0.0.0:5000 app:app\n\n# 2. Use NGINX reverse proxy\n# nginx.conf:\nupstream flask_app {\n    server 127.0.0.1:5000;\n    server 127.0.0.1:5001;\n    server 127.0.0.1:5002;\n}\n\n# 3. Dockerize and use Kubernetes\n# Dockerfile:\nFROM python:3.9\nCOPY . /app\nWORKDIR /app\nRUN pip install -r requirements.txt\nCMD [\"gunicorn\", \"-w\", \"4\", \"-b\", \"0.0.0.0:5000\", \"app:app\"]\n```",
        "mitigation": "Deploy with Gunicorn/uWSGI, use NGINX, containerize with Docker, deploy to Kubernetes/AWS ECS."
    },
    {
        "id": "REL-003",
        "category": "Reliability",
        "severity": "Medium",
        "title": "No Error Recovery for ML Model Failures",
        "file": "ml/forecast.py",
        "lines": "496-515",
        "description": "If Prophet fails, tries SARIMAX. If that fails, tries ARIMA. If that fails, uses baseline. But no retry logic or partial results.",
        "threat": "Cascade failures, no graceful degradation",
        "reproducible_test": "1. Upload dataset with NaN values\n2. Prophet crashes\n3. SARIMAX crashes\n4. ARIMA crashes\n5. Baseline returns empty forecast\n6. User sees 'No data' instead of partial results",
        "impact": "Poor UX, lost insights, user frustration",
        "suggested_fix": "```python\ndef generate_ml_forecast_with_fallback(df, horizon):\n    results = {\n        'forecast': [],\n        'model_used': None,\n        'warnings': []\n    }\n    \n    # Try models in order of sophistication\n    models = [\n        ('Prophet', lambda: fit_prophet_model(df, horizon)),\n        ('SARIMAX', lambda: fit_arima_model(df, horizon, seasonal=True)),\n        ('ARIMA', lambda: fit_arima_model(df, horizon, seasonal=False)),\n        ('Baseline', lambda: fit_baseline_model(df, horizon))\n    ]\n    \n    for model_name, model_func in models:\n        try:\n            predictions, lower, upper = model_func()\n            results['forecast'] = predictions\n            results['model_used'] = model_name\n            return results\n        except Exception as e:\n            results['warnings'].append(f\"{model_name} failed: {str(e)}\")\n            logger.warning(f\"{model_name} failed, trying next model...\")\n    \n    # If all models fail, return historical average\n    results['forecast'] = [df['value'].mean()] * horizon\n    results['model_used'] = 'Historical Average (All models failed)'\n    return results\n```",
        "mitigation": "Add retry logic, return partial results, implement circuit breaker pattern."
    },
    {
        "id": "DATA-001",
        "category": "Data Integrity",
        "severity": "High",
        "title": "No Data Validation After Mapping Changes",
        "file": "app.py",
        "lines": "428-462",
        "description": "/api/save-mapping accepts any column names without validating they exist in the CSV.",
        "threat": "Runtime errors, wrong forecasts, data corruption",
        "reproducible_test": "1. Upload CSV with columns: Date, Amount\n2. Call /api/save-mapping with {\"date\": \"NonExistentColumn\", \"value\": \"Amount\"}\n3. Mapping saved successfully\n4. Call /api/dashboard\n5. Server crashes with KeyError: 'NonExistentColumn'",
        "impact": "Server crashes, wrong data used for forecasts, business decisions based on incorrect data",
        "suggested_fix": "```python\n@app.route('/api/save-mapping', methods=['POST'])\ndef save_mapping():\n    mapping = request.json\n    \n    # Load current CSV to validate columns\n    df = load_data()\n    if df is None:\n        return jsonify({'error': 'No CSV loaded'}), 400\n    \n    available_columns = set(df.columns)\n    \n    # Validate all mapped columns exist\n    for field, column in normalized_mapping.items():\n        if column and column not in available_columns:\n            return jsonify({\n                'error': f\"Column '{column}' not found in CSV\",\n                'available_columns': list(available_columns)\n            }), 400\n    \n    # Validate data types\n    if 'date' in normalized_mapping:\n        try:\n            pd.to_datetime(df[normalized_mapping['date']])\n        except Exception as e:\n            return jsonify({'error': f\"Invalid date column: {e}\"}), 400\n    \n    if 'value' in normalized_mapping:\n        if not pd.api.types.is_numeric_dtype(df[normalized_mapping['value']]):\n            return jsonify({'error': 'Value column must be numeric'}), 400\n    \n    CURRENT_MAPPING = normalized_mapping\n    data_cache.clear()\n    return jsonify({'success': True})\n```",
        "mitigation": "Validate column existence, validate data types, return helpful error messages."
    },
    {
        "id": "FUTURE-001",
        "category": "Future Threat",
        "severity": "High",
        "title": "No Rate Limiting - API Abuse/DoS",
        "file": "app.py",
        "lines": "All API routes",
        "description": "No rate limiting on any endpoint. Attacker can spam requests to exhaust resources.",
        "threat": "DoS attacks, resource exhaustion, high cloud costs",
        "reproducible_test": "1. Write script to call /api/dashboard 1000 times/second\n2. Server CPU/memory maxes out\n3. Legitimate users cannot access service",
        "impact": "Service outage, high costs, poor UX for legitimate users",
        "suggested_fix": "```python\nfrom flask_limiter import Limiter\nfrom flask_limiter.util import get_remote_address\n\nlimiter = Limiter(\n    app=app,\n    key_func=get_remote_address,\n    default_limits=[\"200 per day\", \"50 per hour\"],\n    storage_uri=\"redis://localhost:6379\"\n)\n\n@app.route('/api/dashboard')\n@limiter.limit(\"10 per minute\")\ndef get_dashboard():\n    # ...\n\n@app.route('/api/upload-csv', methods=['POST'])\n@limiter.limit(\"5 per hour\")  # Stricter for expensive operations\ndef upload_csv():\n    # ...\n```",
        "mitigation": "Implement rate limiting with Flask-Limiter, use Redis for distributed rate limiting, add CAPTCHA for public endpoints."
    },
    {
        "id": "FUTURE-002",
        "category": "Future Threat",
        "severity": "Medium",
        "title": "No Audit Logging - Compliance/Forensics Gap",
        "file": "app.py",
        "lines": "All",
        "description": "No logging of user actions (uploads, downloads, data access). Cannot trace security incidents or meet compliance requirements.",
        "threat": "Cannot investigate breaches, GDPR/SOC2 compliance failures, no accountability",
        "reproducible_test": "1. User uploads sensitive data\n2. User downloads forecast\n3. User deletes data\n4. No record of any of these actions exists",
        "impact": "Compliance violations, inability to investigate incidents, no audit trail",
        "suggested_fix": "```python\nimport logging\nfrom datetime import datetime\n\naudit_logger = logging.getLogger('audit')\naudit_logger.setLevel(logging.INFO)\nhandler = logging.FileHandler('audit.log')\naudit_logger.addHandler(handler)\n\ndef log_audit_event(user_id, action, resource, details=None):\n    audit_logger.info(json.dumps({\n        'timestamp': datetime.utcnow().isoformat(),\n        'user_id': user_id,\n        'action': action,\n        'resource': resource,\n        'details': details,\n        'ip': request.remote_addr,\n        'user_agent': request.user_agent.string\n    }))\n\n@app.route('/api/upload-csv', methods=['POST'])\ndef upload_csv():\n    # ... existing code ...\n    log_audit_event(\n        user_id=session.get('user_id'),\n        action='UPLOAD',\n        resource=filename,\n        details={'size': file_size, 'rows': len(df)}\n    )\n```",
        "mitigation": "Implement audit logging, use centralized logging (ELK, Splunk), retain logs for compliance period (7 years for some regulations)."
    },
    {
        "id": "MAINT-002",
        "category": "Maintainability",
        "severity": "Low",
        "title": "No API Versioning - Breaking Changes Risk",
        "file": "app.py",
        "lines": "All API routes",
        "description": "API routes have no version prefix (/api/v1/). Future changes will break existing clients.",
        "threat": "Cannot evolve API without breaking clients, poor developer experience",
        "reproducible_test": "1. Mobile app uses /api/dashboard\n2. You change response format\n3. Mobile app breaks for all users",
        "impact": "Cannot deploy updates without coordinating with all clients, slow iteration",
        "suggested_fix": "```python\n# Version 1 (current)\n@app.route('/api/v1/dashboard')\ndef get_dashboard_v1():\n    # ... existing code ...\n\n# Version 2 (new format)\n@app.route('/api/v2/dashboard')\ndef get_dashboard_v2():\n    # ... new format ...\n\n# Redirect /api/dashboard to latest version\n@app.route('/api/dashboard')\ndef get_dashboard():\n    return get_dashboard_v2()\n```",
        "mitigation": "Add /api/v1/ prefix to all routes, maintain old versions for 6-12 months, document deprecation timeline."
    }
]